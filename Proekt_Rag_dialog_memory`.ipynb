{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swYaMC1mCYUh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Установка библиотек\n",
        "!pip install -U langchain langchain-google-genai google-generativeai chromadb sentence-transformers langchain-community beautifulsoup4 html2text\n",
        "\n",
        "from langsmith import Client\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "import google.generativeai as genai\n",
        "from langchain.memory import ConversationBufferMemory         #memory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# Загрузка и разбиение документов\n",
        "urls = [\n",
        "    \"https://www.euronews.com/news/europe\",\n",
        "    \"https://www.dw.com/en/top-stories/s-9097\",\n",
        "    \"https://www.reuters.com/news/archive/worldNews\",\n",
        "    \"https://apnews.com/hub/europe\",\n",
        "    \"https://www.bbc.com/news/world/europe\",\n",
        "    \"https://en.wikipedia.org/wiki/History_of_Europe\",\n",
        "    \"https://european-union.europa.eu/news-and-events_en\"\n",
        "]\n",
        "loader = WebBaseLoader(urls)                     #ручной ask_rag() на LangChain-цепочку ConversationalRetrievalChain, в которую встроим память и retriever.\n",
        "docs = loader.load()\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\", return_messages=True\n",
        ")\n",
        "\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    return_source_documents=True,  # можно отключить, если не нужно\n",
        "    verbose=True  # помогает при отладке\n",
        ")\n",
        "\n",
        "\n",
        "docs.append(\n",
        "    Document(\n",
        "        page_content=\"Sturm Eowyn ereignete sich in Österreich im Dezember 2024.\",\n",
        "        metadata={\"source\": \"manually\"}))\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "split_docs = splitter.split_documents(docs)\n",
        "\n",
        "# Настройка API ключей и логирования\n",
        "load_dotenv(\".env\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "class Config:\n",
        "    @staticmethod\n",
        "    def setup():\n",
        "        langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "        if langchain_api_key:\n",
        "            os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
        "        langsmith_endpoint = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
        "        if langsmith_endpoint:\n",
        "            os.environ[\"LANGCHAIN_ENDPOINT\"] = langsmith_endpoint\n",
        "        langsmith_project = os.getenv(\"LANGSMITH_PROJECT\")\n",
        "        if langsmith_project:\n",
        "            os.environ[\"LANGCHAIN_PROJECT\"] = langsmith_project\n",
        "\n",
        "Config.setup()\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# Векторизация и сохранение\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(documents=split_docs, embedding=embedding, persist_directory=\"chroma_storage\")\n",
        "vectordb.persist()\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# Создание LLM и цепочки с памятью\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.4)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, return_source_documents=True, verbose=True)\n",
        "\n",
        "# Функция для диалога\n",
        "def ask_rag_chat(question):\n",
        "    result = qa_chain({\"question\": question})\n",
        "    answer = result[\"answer\"]\n",
        " #   print(f\"\\nQ: {question}\\nA: {answer}\")\n",
        "    print(f\"Q: {question}\\nA: {answer}\")\n",
        "    return answer\n",
        "\n",
        "# Пример диалога\n",
        "ask_rag_chat(\"Was war Sturm Eowyn?\")\n",
        "ask_rag_chat(\"Wo genau in Österreich?\")\n",
        "ask_rag_chat(\"Gab es Verletzte?\")\n",
        "\n",
        "# Сохраняем файл\n",
        "file_path = Path(\"/mnt/data/rag_project_with_memory.py\")\n",
        "file_path.write_text(code_with_memory)\n",
        "\n",
        "file_path.name  # Название сгенерированного файла для пользователя\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
