{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**üìö RAG-System mit LangChain, ChromaDB und Gemini 2**\n",
        "\n",
        "---\n",
        "\n",
        "Dieses Notebook implementiert ein einfaches Retrieval-Augmented Generation (RAG) System.\n",
        "Es verwendet ChromaDB zur Dokumentenspeicherung, LangChain f√ºr Workflow-Management und das Modell gemini-2.0-flash.\n",
        "Das System l√§dt ein Dokument, unterteilt es in kleinere Chunks, speichert sie in einer Vektor-Datenbank\n",
        "und nutzt Google Gemini zur Generierung von Antworten auf benutzerdefinierte Fragen unter Ber√ºcksichtigung von Kontext und vorherigen Gespr√§chen.`\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-QBFFuU0X8of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ‚úÖ Installieren aller Abh√§ngigkeiten.Zuerst installieren wir alle notwendigen Bibliotheken, die f√ºr das RAG-System ben√∂tigt werden.Dazu geh√∂ren LangChain, LangChain Community, Google Gemini und ChromaDB.\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pibdF3VVYkrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y google-generativeai\n",
        "!pip install -q langchain-google-genai==2.1.4 google-ai-generativelanguage==0.6.18\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDFCVrgDSfFZ",
        "outputId": "23c882c4-e331-4279-85a8-c9a2d900c3a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: google-generativeai 0.8.5\n",
            "Uninstalling google-generativeai-0.8.5:\n",
            "  Successfully uninstalled google-generativeai-0.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pDqp-yU9b3dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community langchain-google-genai langgraph chromadb"
      ],
      "metadata": {
        "id": "wt6ZQEqLSlgE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Bibliotheken importieren.In diesem Abschnitt importieren wir alle erforderlichen Bibliotheken.Dies umfasst LangChain f√ºr die Dokumentenverarbeitung, Google Gemini f√ºr das Modell, und ChromaDB f√ºr die Speicherung und den Abruf von Vektor-Daten.\n",
        "---"
      ],
      "metadata": {
        "id": "OehbSpWSYyM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory.chat_message_histories import FileChatMessageHistory\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.callbacks.tracers import LangChainTracer\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import TypedDict, List\n",
        "from langchain import hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrZm8r2IStag",
        "outputId": "7d01bd08-5e85-4550-bd46-54842fb42fa2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Einstellen der Umgebungsvariablen.Hier setzen wir die Umgebungsvariablen f√ºr die API-Schl√ºssel von LangChain und Google Gemini.Diese Schl√ºssel werden ben√∂tigt, um die Funktionen des Systems zu aktivieren.\n",
        "---"
      ],
      "metadata": {
        "id": "Bpq2n82BY8eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"üîëLangSmith API Key: \")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG.2025\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass(\"üîëAPI Key (Gemini): \")\n",
        "os.environ[\"USER_AGENT\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/123.0.0.0 Safari/537.36\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxqZrZr1TJTq",
        "outputId": "2ac313bd-44ac-431d-94e6-238f364ef7a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîëLangSmith API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "üîëAPI Key (Gemini): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Laden des Dokuments (nach August 2024).In diesem Schritt laden wir das Dokument von der angegebenen URL und extrahieren den Text.Die URL verweist auf eine Seite, die Informationen zum r√ºckl√§ufigen Mars in der Astrologie enth√§lt.\n",
        "---"
      ],
      "metadata": {
        "id": "BbLbypyvZFBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.astromind.de/astrologie-artikel/r%C3%BCcklaeufiger-mars.html\"\n",
        "loader = WebBaseLoader(url)\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "il4-tFi0TU0a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Dokument in Chunks aufteilen.Um das Dokument effizient zu verarbeiten und die Antwortqualit√§t zu verbessern, teilen wir es in kleinere Chunks auf.Dies erleichtert das Abrufen relevanter Informationen f√ºr spezifische Anfragen.\n",
        "---"
      ],
      "metadata": {
        "id": "ImlRw9BUZezH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(documents)\n",
        "print(f\"üîπ –°hunks: {len(chunks)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY2SSZYxTbS-",
        "outputId": "a6fcd560-d347-42f0-b634-110d33a8ba26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ –°hunks: 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Vektorisierung und Speicherung.Hier vektorisieren wir die Chunks des Dokuments mit einem vortrainierten Modell von HuggingFace.Anschlie√üend speichern wir die Vektoren in einer Chroma-Datenbank, die als persistente Speicherung dient.\n",
        "---"
      ],
      "metadata": {
        "id": "X1bBghXMZqIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "persist_directory = \"chroma_db\"\n",
        "\n",
        "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "    vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding)   # Wenn die Chroma-Datenbank bereits existiert, laden wir sie.\n",
        "else:\n",
        "    vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=persist_directory) # Ansonsten erstellen wir die Chroma-Datenbank und speichern die Vektoren.\n",
        "    vectorstore.persist()\n"
      ],
      "metadata": {
        "id": "42UvnYLcU4ql"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Einrichtung der Speicher- und Tracking-Funktionen.In diesem Schritt richten wir die Speicherung des Chat-Verlaufs und die Tracking-Funktion ein,die es uns erm√∂glicht, die Anfragen und Antworten in unserem RAG-System nachzuverfolgen.\n",
        "---"
      ],
      "metadata": {
        "id": "Esj2J8JDaDjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory.chat_message_histories import FileChatMessageHistory\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–º—è—Ç–∏\n",
        "message_history = FileChatMessageHistory(\"chat_history.json\")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    chat_memory=message_history,\n",
        "    return_messages=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "R70XbMq-VE3J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Einrichtung des Modells und Retrievers.Wir konfigurieren das generative Modell von Google (Gemini) und den Multi-Query Retriever,der mehrere verwandte Dokumente f√ºr jede Anfrage abruft, um genauere Antworten zu generieren.\n",
        "---"
      ],
      "metadata": {
        "id": "EB24tO0maRo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    llm=llm\n",
        ")"
      ],
      "metadata": {
        "id": "f6_s5-w4VVvJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Abrufen des Prompts aus LangChain Hub.Hier rufen wir das Prompt aus dem LangChain Hub ab, das als Vorlage f√ºr das Generieren von Antworten dient.\n",
        "---"
      ],
      "metadata": {
        "id": "NvZ7ZjBXaZsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "WIwXn6oiVgYF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ‚úÖ Definition des Zustands und der Funktionen.Wir definieren den Zustand des Systems und zwei Funktionen:- \"retrieve\" zum Abrufen relevanter Dokumente.- \"generate\" zur Generierung der Antwort unter Verwendung des Modells und der gespeicherten Kontexte.\n",
        " ---"
      ],
      "metadata": {
        "id": "DgLX9up5ahum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def retrieve(state: State):\n",
        "    docs = multi_query_retriever.get_relevant_documents(state[\"question\"])\n",
        "    return {\"context\": docs}\n",
        "\n",
        "def generate(state: State):\n",
        "    history = memory.load_memory_variables({})[\"chat_history\"]\n",
        "    custom_instruction = \"Antworte auf Deutsch.\"\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    full_context = f\"{custom_instruction}\\n\\n{history}\\n\\nDokumente:\\n{docs_content}\"\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": full_context})    # Abrufen der Antwort mit dem generativen Modell\n",
        "    response = llm.invoke(messages, config={\"callbacks\": [tracer]})\n",
        "    memory.save_context(inputs={\"input\": state[\"question\"]}, outputs={\"output\": response.content})    # Speichern der Antwort im Verlauf\n",
        "    return {\"answer\": response.content}"
      ],
      "metadata": {
        "id": "fiHLvBgzVkF3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Erstellen des Zustandsgraphen.Wir bauen den Graphen f√ºr die Zustands√ºberg√§nge, wobei wir die Funktionen \"retrieve\" und \"generate\" verkn√ºpfen.\n",
        "---"
      ],
      "metadata": {
        "id": "CdWFRcPXa_X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "iKg41clWVqul"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Beispiel f√ºr die Ausf√ºhrung.In diesem Beispiel stellen wir eine Frage an das System und generieren die Antwort basierend auf dem Kontext.\n",
        "---"
      ],
      "metadata": {
        "id": "p4cK1Gc_bNMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞\n",
        "state = {\"question\": \"Was bedeutet ein r√ºckl√§ufiger Mars?\"}\n",
        "result = graph.invoke(state)\n",
        "print(\"üìçAntwort:\", result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7WNt1g7VuDv",
        "outputId": "d7d7ff37-1f58-4110-bd95-9879da7f1e10"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìçAntwort: Ein r√ºckl√§ufiger Mars bedeutet, dass der Planet von der Erde aus betrachtet r√ºckw√§rts durch den Himmel zu wandern scheint. In der Astrologie wird dies als eine Zeit beschrieben, in der die Energie des Planeten nach innen gelenkt wird. F√ºr Mars, der normalerweise f√ºr Antrieb, Mut und Tatendrang steht, kann das bedeuten, dass man verlangsamt wird und vor un√ºberlegten Entscheidungen gesch√ºtzt wird.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖBeispiel-Dialog\n",
        "----"
      ],
      "metadata": {
        "id": "dDZVdPtIbZfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"Warum ist das bedeutsam f√ºr den Jahresbeginn?\",\n",
        "    \"Was sollte man bei der Planung von Aktivit√§ten w√§hrend des r√ºckl√§ufigen Mars beachten?\",\n",
        "    \"Welche Empfehlungen gibt die Astrologie, um die negativen Effekte des r√ºckl√§ufigen Mars zu minimieren?\",\n",
        "    \"Wie beeinflusst der r√ºckl√§ufige Mars das Privatleben?\",\n",
        "    \"Welche Fehler sollte man w√§hrend des r√ºckl√§ufigen Mars vermeiden?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = graph.invoke({\"question\": q})\n",
        "    print(f\"‚ùì Frage: {q}\")\n",
        "    print(f\"‚úÖ Antwort: {result['answer']}\\n{'-'*80}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRPgYSf9WSb3",
        "outputId": "0d626e05-28c3-40d8-9b69-19d09c9f88ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùì Frage: Warum ist das bedeutsam f√ºr den Jahresbeginn?\n",
            "‚úÖ Antwort: Der r√ºckl√§ufige Mars zu Jahresbeginn 2025 bedeutet, dass man nicht √ºberst√ºrzt mit neuen Ideen ins Jahr starten sollte. Es ist eine Zeit, um Altes loszulassen und Klarheit dar√ºber zu gewinnen, was man wirklich will. Dies sch√ºtzt vor un√ºberlegten Entscheidungen und erm√∂glicht es, sich auf das Wesentliche zu konzentrieren.\n",
            "--------------------------------------------------------------------------------\n",
            "‚ùì Frage: Was sollte man bei der Planung von Aktivit√§ten w√§hrend des r√ºckl√§ufigen Mars beachten?\n",
            "‚úÖ Antwort: W√§hrend des r√ºckl√§ufigen Mars sollte man es vermeiden, √ºberst√ºrzt mit neuen Ideen zu starten und sich stattdessen darauf konzentrieren, Altes loszulassen und Klarheit zu gewinnen. Man sollte sich innerlich flexibel halten und offen f√ºr Ver√§nderungen sein, falls man in dieser Zeit mit etwas Neuem beginnen muss. Es ist ratsam, innovative Vorhaben aufzuschieben, bis die Phase des r√ºckl√§ufigen Mars vorbei ist.\n",
            "--------------------------------------------------------------------------------\n",
            "‚ùì Frage: Welche Empfehlungen gibt die Astrologie, um die negativen Effekte des r√ºckl√§ufigen Mars zu minimieren?\n",
            "‚úÖ Antwort: Die Astrologie empfiehlt, w√§hrend des r√ºckl√§ufigen Mars keine √ºberst√ºrzten neuen Ideen zu beginnen, sondern sich darauf zu konzentrieren, Altes loszulassen und Klarheit zu gewinnen. Es wird geraten, sich innerlich flexibel zu halten und innovative Vorhaben aufzuschieben, bis die Phase vorbei ist. Dies soll vor un√ºberlegten Entscheidungen sch√ºtzen und erm√∂glichen, sich auf das Wesentliche zu konzentrieren.\n",
            "--------------------------------------------------------------------------------\n",
            "‚ùì Frage: Wie beeinflusst der r√ºckl√§ufige Mars das Privatleben?\n",
            "‚úÖ Antwort: Der bereitgestellte Kontext enth√§lt keine spezifischen Informationen dar√ºber, wie ein r√ºckl√§ufiger Mars das Privatleben beeinflusst. Es wird lediglich beschrieben, dass ein r√ºckl√§ufiger Mars die Energie des Planeten nach innen lenkt und vor un√ºberlegten Entscheidungen sch√ºtzt. Dies kann dazu beitragen, Klarheit dar√ºber zu gewinnen, was man wirklich will.\n",
            "--------------------------------------------------------------------------------\n",
            "‚ùì Frage: Welche Fehler sollte man w√§hrend des r√ºckl√§ufigen Mars vermeiden?\n",
            "‚úÖ Antwort: W√§hrend des r√ºckl√§ufigen Mars sollte man es vermeiden, √ºberst√ºrzt mit neuen Ideen zu starten. Konzentriere dich stattdessen darauf, Altes loszulassen und Klarheit zu gewinnen. Es ist ratsam, innovative Vorhaben aufzuschieben, bis die Phase des r√ºckl√§ufigen Mars vorbei ist.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}